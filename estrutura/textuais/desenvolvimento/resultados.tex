\chapter{RESULTADOS}
\label{chap:results}

Este capítulo é dedicado aos resultados obtidos com testes
feitos utilizando a solução desenvolvida no trabalho. Os testes
utilizaram um ambiente de VM para que os testes possam ser reproduzidos 
mais fácilmente. Foram utilizadas diferentes métricas para avaliar
o comportamento da API. Além das diferentes métricas, os dados inseridos 
também possuem diferentes formatos e valores. Isso foi feito para que 
seja percebido o comportamento da API em diferentes cenários, assim como
avaliar se o comportamento é alterado drásticamente ou mantem seu comportamento
mesmo em casos extremos.

É importante definir bem como a API se comporta em cenários diferentes, pois,
dependendo aonde ela estiver instalada, cada requisição gera um custo. Por exemplo, se a API 
estiver hospedada em nuvem, cada requisição tem um custo, seja ele por tráfego de
rede, armazenamento dos dados, processamento, etc. 

\section{AMBIENTE}

O ambiente utilizado foi uma máquina virtual utilizando \textbf{Ubuntu Server 18.04}
com memória de \textbf{4.0GB} utilizando um processador \textbf{2,3 GHz Intel Core i5}. 
As requisições são feitas através de uma rede \textit{bridge} da máquina física para a virtual.

A escolha de um ambiente com VM é permitir o isolamento de recursos. Além disso,
o ambiente é mantido de forma mais controlada, sendo assim, há uma melhor 
consistencia dos testes feitos. Além do maior controle sobre o ambiente, é importante salientar 
que o ambiente simulado está disponível nas nuvens comerciais na indústria, como AWS, AZURE e 
etc. Sendo assim, caso uma máquina com as características da VM testada for utilizada, 
o desempenho da API tende a ser o mesmo.

\section{DADOS UTILIZADOS}
A API do OptVM permite que seja feitas várias combinações de dados. As combinações
podem variar em relação à restrições utilizadas, objetivos da otimização, quantidade
de hosts, nuvens e datacenters envolvidos, entre outros. 

Na avaliação dos resultados, foram utilizados dados que simulam casos simples e casos mais 
complexos. O formato dos cenários utilizados é demonstrado na Tabela \ref{tab:testdata-info}

\input{dados/tabelas/testdata-info.tex}

Os dados utilizados foram gerados através de um código java. O código
construído gerou em todos os casos 5 nuvens, sendo que, a quantidade de
datacenters/hosts varia para ficar compatível com cada cenário. Para cada
host, atribuiu-se 5 VMs.

Para os testes conter os dados mais próximos da realidade quanto possível,
foram pré-definidos alguns valores para atributos que são fixos, por exemplo,
sistema operacional, memória RAM total, armazenamento total. Nestes casos, um dos valores
pré-definidos foi selecionado aleatoriamente como valor para o atributo da entidade. 
Nos casos onde o atributo é completamente variável, foi atribuido valores aleatórios para os 
mesmos mantendo um intervalo condizente com a realidade, por exemplo, preço por armazenamento
entre 0.1 e 1 (centavos).

Além dos dados da nuvem, DCs e hosts, foi utilizado como padrão a busca de um subconjunto de
3 hosts como sendo uma possível solução. 

\section{MÉTRICAS UTILIZADAS}
É muito comum uma API ser medida através de uma medida quantitativa de requisições 
por segundo (RPS) que ela consegue atender. Porém, o número de requisições 
varia, e isso depende da tecnologia utilizada, arquitetura do sistema, se há 
consulta em banco de dados ou não, algoritmos utilizado, etc.

Além da métrica de requisições por segundo, outro aspecto importante na utilização de API
é o tamanho do \textit{payload}. O payload, muitas vezes é atribuido um custo por 
tráfego da máquina, e por processamento e armazenamento, sendo assim, quanto maior
o payload, mais tráfego, processamento e armazenamento ele gerará, consequentemente,
aumentará o custo. 

\subsection{WRK}

O WRK é uma ferramenta construída em C utilizada para fazer \textit{benchmarking} de 
aplicações HTTP. Com ela é possível fazer requisições concorrentes utilizando 
múltiplos ou um único core. Além disso, o WRK permite utilizar uma duração
do teste, threads e conexões HTTP. Para o teste, foram utilizados os seguintes 
valores para cada parâmetro:

\begin{enumerate}
  \item Conexões: 25
  \item Threads: 1
  \item Tempo de teste: 3 minutos
\end{enumerate}

Os parâmetros escolhidos, significam que uma thread está disparando requisições através 
de 25 conexões para o serviço. Ou seja, o serviço recebe as requisições através de 25 conexões
concorrentes. Já o tempo escolhido, foi de três minutos, para que não haja um viés no número
de requisições por conta de alguma lentidão inicial, por exemplo, abertura de conexão com 
banco de dados.

\section{COMPARAÇÕES}
